{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h2>INFSCI 2595 Machine Learning - Spring 2019 </h2>\n",
    "<h1 style=\"font-size: 250%;\">Assignment #2</h1>\n",
    "<h3>Total points: 100 </h3>\n",
    "<h2> Due by April 1st </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type in your information in the double quotes\n",
    "firstName = \"Shanim\"\n",
    "lastName = \"Manzoor\"\n",
    "pittID = \"shm150\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <h3>  Problem #1. Linear Discriminant Analysis (LDA)and Quadratic Discriminant Analysis(QDA) </h3> \n",
    " ### [30 points]\n",
    " \n",
    "Do not write a code for this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-1 [10 points]</h4>  <br>\n",
    "We have a classification problem with one feature $(x)$ and K classes to be classified. The prior probability of\n",
    "class $k$ is $ùúã_{k} = ùëÉ(ùëå = ùëò)$. Assume that the feature in class k has Gaussian distribution of\n",
    "mean $Œº_{k}$ and variance $œÉ^2 (ùí©(Œº,ùúé^{2}))$.The variance is the same for all classes. \n",
    "Prove that the Bayes‚Äô classifier (that chooses class k with largest $ùëÉ(ùëå = ùëò|ùë•))$ is equivalent to assigning an observation to the class for which the discriminant function $ùõø_{k}(x)$ is\n",
    "maximized, where \n",
    "\\begin{array} \\\\\n",
    "ùõø_{k}(x) = x\\frac{\\mu _{k}}{\\sigma ^{2}}- \\frac{\\mu_{2}^{k}}{2\\sigma ^{2}}+ log(\\pi _{k})\n",
    "\\end{array}\n",
    "<br> What is the name of this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/file1.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-2 [15 points]</h4>  <br>\n",
    "Extend **Problem #1-1** to include **p** features. With features from each class drawn from a\n",
    "Gaussian distribution with mean vector $Œº_{k}$ and covariance matrix $Œ£_{k}$ (which is now\n",
    "different for each class). What is the discriminant function that maximizes **ùëÉ(ùëå = ùëò|ùë•)**. Is\n",
    "the relationship with the feature vector **x** linear?<br> What is this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/file22.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-3 [5 points]</h4>  <br>\n",
    "- Explain Bias and variance trade-off between the two above classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LDA(Linear Discriminant Analysis) we see that if there are p predictors, it still assumes that the covariance matrix is same for all. If the variance is actually uniform everywhere, this would be correct and computation needed is much lower. But if the assumption is wrong , the results would be very poor. Hence it is much LESS FLEXIBLE AND HAS LESS VARIANCE BUT HAS HIGH BIAS.\n",
    "In QDA (Quadratic Discriminant Analysis) we have separate co-variance matrix for each class which implies more computation . This model is hence MORE FLEXIBLE WHICH IMPLIES HIGH VARIANCE BUT LESS BIAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <h3>  Problem #2. Regularization    </h3>\n",
    "### [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-1. [3 points]</h4> Answer the following questions \n",
    "\n",
    "- What is the main purpose of using regularization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of regularization is to reduce the complexity of a model. This in turn decreases overfitting. Regularization works by adding a penalty term to our objective function  to control the complexity. This decreases variance of the model .\n",
    "Further, we see that when the dataset has a huge number of features, while the number of instances are low in comparison or insufficient, we see that high variance starts coming into play. This can be controlled by regularization. It therefore decreases variance and increases bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-2.Logistic Regression with Ridge Regularization [12 points] </h4>  <br>\n",
    "\n",
    "In this part, you should download and analyze the Wisconson **\"breast_cancer\"** dataset. <br>\n",
    "\n",
    "- Fit logistic regression model using ridge regularization with different values of  C = 0.1, 1, 5, 10, 50, 100, and 1000 (Note that C is the LogisticRegression function argument). For each value, report the estimated coefficients for the fitted model (do not just print summary, make a table with feature names and estimated coefficients)\n",
    "\n",
    "- What happens to the coefficients as you increase C?\n",
    "\n",
    "- What happens to the flexibility of the model as you increase C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shani\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\shani\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\shani\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\shani\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\shani\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\shani\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\shani\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.665411</td>\n",
       "      <td>0.087508</td>\n",
       "      <td>0.310579</td>\n",
       "      <td>-0.011101</td>\n",
       "      <td>-0.024660</td>\n",
       "      <td>-0.112482</td>\n",
       "      <td>-0.159466</td>\n",
       "      <td>-0.068620</td>\n",
       "      <td>-0.034316</td>\n",
       "      <td>-0.006372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679468</td>\n",
       "      <td>-0.228427</td>\n",
       "      <td>-0.212196</td>\n",
       "      <td>-0.015818</td>\n",
       "      <td>-0.045413</td>\n",
       "      <td>-0.346369</td>\n",
       "      <td>-0.433008</td>\n",
       "      <td>-0.132559</td>\n",
       "      <td>-0.109339</td>\n",
       "      <td>-0.032552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2.101333</td>\n",
       "      <td>0.122048</td>\n",
       "      <td>-0.057356</td>\n",
       "      <td>-0.003594</td>\n",
       "      <td>-0.153787</td>\n",
       "      <td>-0.399851</td>\n",
       "      <td>-0.643594</td>\n",
       "      <td>-0.340199</td>\n",
       "      <td>-0.225912</td>\n",
       "      <td>-0.026004</td>\n",
       "      <td>...</td>\n",
       "      <td>1.245314</td>\n",
       "      <td>-0.346269</td>\n",
       "      <td>-0.125742</td>\n",
       "      <td>-0.023993</td>\n",
       "      <td>-0.284648</td>\n",
       "      <td>-1.118689</td>\n",
       "      <td>-1.571928</td>\n",
       "      <td>-0.653832</td>\n",
       "      <td>-0.690302</td>\n",
       "      <td>-0.112870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>3.806946</td>\n",
       "      <td>0.165450</td>\n",
       "      <td>-0.316113</td>\n",
       "      <td>-0.007404</td>\n",
       "      <td>-0.535809</td>\n",
       "      <td>-0.662065</td>\n",
       "      <td>-1.431558</td>\n",
       "      <td>-1.068888</td>\n",
       "      <td>-0.755004</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755611</td>\n",
       "      <td>-0.455093</td>\n",
       "      <td>-0.004469</td>\n",
       "      <td>-0.024508</td>\n",
       "      <td>-1.021818</td>\n",
       "      <td>-1.440923</td>\n",
       "      <td>-2.814447</td>\n",
       "      <td>-2.022093</td>\n",
       "      <td>-1.989563</td>\n",
       "      <td>-0.104313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>4.399252</td>\n",
       "      <td>0.196342</td>\n",
       "      <td>-0.376778</td>\n",
       "      <td>-0.011085</td>\n",
       "      <td>-0.649935</td>\n",
       "      <td>-0.792452</td>\n",
       "      <td>-1.701317</td>\n",
       "      <td>-1.279704</td>\n",
       "      <td>-0.928721</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398140</td>\n",
       "      <td>-0.515016</td>\n",
       "      <td>0.053002</td>\n",
       "      <td>-0.023936</td>\n",
       "      <td>-1.239660</td>\n",
       "      <td>-1.763487</td>\n",
       "      <td>-3.339636</td>\n",
       "      <td>-2.427975</td>\n",
       "      <td>-2.459531</td>\n",
       "      <td>-0.133186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.0</th>\n",
       "      <td>4.608842</td>\n",
       "      <td>0.234913</td>\n",
       "      <td>-0.385851</td>\n",
       "      <td>-0.014415</td>\n",
       "      <td>-0.674125</td>\n",
       "      <td>-0.875968</td>\n",
       "      <td>-1.797327</td>\n",
       "      <td>-1.315552</td>\n",
       "      <td>-0.979580</td>\n",
       "      <td>-0.011748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125515</td>\n",
       "      <td>-0.570265</td>\n",
       "      <td>0.090930</td>\n",
       "      <td>-0.022488</td>\n",
       "      <td>-1.283905</td>\n",
       "      <td>-2.100238</td>\n",
       "      <td>-3.661979</td>\n",
       "      <td>-2.513409</td>\n",
       "      <td>-2.656133</td>\n",
       "      <td>-0.175657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>5.891077</td>\n",
       "      <td>0.189023</td>\n",
       "      <td>-0.518216</td>\n",
       "      <td>-0.017789</td>\n",
       "      <td>-1.246230</td>\n",
       "      <td>-0.870716</td>\n",
       "      <td>-2.309676</td>\n",
       "      <td>-2.392791</td>\n",
       "      <td>-1.644340</td>\n",
       "      <td>0.127627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099841</td>\n",
       "      <td>-0.529985</td>\n",
       "      <td>0.049821</td>\n",
       "      <td>-0.018259</td>\n",
       "      <td>-2.418720</td>\n",
       "      <td>-1.094326</td>\n",
       "      <td>-2.969409</td>\n",
       "      <td>-4.439373</td>\n",
       "      <td>-3.791072</td>\n",
       "      <td>0.060051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.0</th>\n",
       "      <td>5.901814</td>\n",
       "      <td>0.140402</td>\n",
       "      <td>-0.430363</td>\n",
       "      <td>-0.024759</td>\n",
       "      <td>-3.670967</td>\n",
       "      <td>0.233504</td>\n",
       "      <td>-4.288088</td>\n",
       "      <td>-6.835734</td>\n",
       "      <td>-3.846296</td>\n",
       "      <td>0.821566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.643605</td>\n",
       "      <td>-0.493568</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>-0.008779</td>\n",
       "      <td>-7.541171</td>\n",
       "      <td>3.422716</td>\n",
       "      <td>-3.328733</td>\n",
       "      <td>-12.994694</td>\n",
       "      <td>-6.759584</td>\n",
       "      <td>1.063963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0.1        0.665411      0.087508        0.310579  -0.011101        -0.024660   \n",
       "1.0        2.101333      0.122048       -0.057356  -0.003594        -0.153787   \n",
       "5.0        3.806946      0.165450       -0.316113  -0.007404        -0.535809   \n",
       "10.0       4.399252      0.196342       -0.376778  -0.011085        -0.649935   \n",
       "50.0       4.608842      0.234913       -0.385851  -0.014415        -0.674125   \n",
       "100.0      5.891077      0.189023       -0.518216  -0.017789        -1.246230   \n",
       "1000.0     5.901814      0.140402       -0.430363  -0.024759        -3.670967   \n",
       "\n",
       "        mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0.1            -0.112482       -0.159466            -0.068620      -0.034316   \n",
       "1.0            -0.399851       -0.643594            -0.340199      -0.225912   \n",
       "5.0            -0.662065       -1.431558            -1.068888      -0.755004   \n",
       "10.0           -0.792452       -1.701317            -1.279704      -0.928721   \n",
       "50.0           -0.875968       -1.797327            -1.315552      -0.979580   \n",
       "100.0          -0.870716       -2.309676            -2.392791      -1.644340   \n",
       "1000.0          0.233504       -4.288088            -6.835734      -3.846296   \n",
       "\n",
       "        mean fractal dimension           ...             worst radius  \\\n",
       "0.1                  -0.006372           ...                 0.679468   \n",
       "1.0                  -0.026004           ...                 1.245314   \n",
       "5.0                   0.004385           ...                 0.755611   \n",
       "10.0                  0.001637           ...                 0.398140   \n",
       "50.0                 -0.011748           ...                 0.125515   \n",
       "100.0                 0.127627           ...                -0.099841   \n",
       "1000.0                0.821566           ...                -0.643605   \n",
       "\n",
       "        worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "0.1         -0.228427        -0.212196   -0.015818         -0.045413   \n",
       "1.0         -0.346269        -0.125742   -0.023993         -0.284648   \n",
       "5.0         -0.455093        -0.004469   -0.024508         -1.021818   \n",
       "10.0        -0.515016         0.053002   -0.023936         -1.239660   \n",
       "50.0        -0.570265         0.090930   -0.022488         -1.283905   \n",
       "100.0       -0.529985         0.049821   -0.018259         -2.418720   \n",
       "1000.0      -0.493568         0.005924   -0.008779         -7.541171   \n",
       "\n",
       "        worst compactness  worst concavity  worst concave points  \\\n",
       "0.1             -0.346369        -0.433008             -0.132559   \n",
       "1.0             -1.118689        -1.571928             -0.653832   \n",
       "5.0             -1.440923        -2.814447             -2.022093   \n",
       "10.0            -1.763487        -3.339636             -2.427975   \n",
       "50.0            -2.100238        -3.661979             -2.513409   \n",
       "100.0           -1.094326        -2.969409             -4.439373   \n",
       "1000.0           3.422716        -3.328733            -12.994694   \n",
       "\n",
       "        worst symmetry  worst fractal dimension  \n",
       "0.1          -0.109339                -0.032552  \n",
       "1.0          -0.690302                -0.112870  \n",
       "5.0          -1.989563                -0.104313  \n",
       "10.0         -2.459531                -0.133186  \n",
       "50.0         -2.656133                -0.175657  \n",
       "100.0        -3.791072                 0.060051  \n",
       "1000.0       -6.759584                 1.063963  \n",
       "\n",
       "[7 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no split done as not requested\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "cancer1 = load_breast_cancer()\n",
    "coef_list = []\n",
    "C_grid = [0.1, 1, 5, 10, 50, 100, 1000]\n",
    "for c in C_grid:\n",
    "    logistic = LogisticRegression(C = c).fit(cancer1.data, cancer1.target)\n",
    "    coef_list.append(logistic.coef_.tolist()[0])\n",
    "coef_df = pd.DataFrame(coef_list, columns=cancer1.feature_names , index = C_grid )\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment:\n",
    " -as you increase C: C is inverse of the lambda parameter( logistic regression) , so we see that lower the C more the regularization. But once we INCREASE C, we can see the penalty becomes low and coefficient values increases(absolute). We can see this by comparing first and larst row of the table printed above this comment.\n",
    "        \n",
    "-Flexibility:\n",
    "As C increases , we see that regularization effect decreases and hence flexibility increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3>  Problem #3. Logistic Regression and Unbalanced Datasets  </h3> \n",
    "### [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We fit a logistic regression model to predict the probability that an individual will default on his/her credit card balance. We used the total balance (single feature) to fit the model and got the results shown in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-1. [5 points]</h4> Prediciton with Logistic regression <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# |Table|Coefficient|Std.error|Z-statistic|P-Value|\n",
    "|:--:|:-------------------------------:|\n",
    "|Intercept|-10.6513|0.3612|-29.5|<0.0001|\n",
    "|balance|0.0055|0.002|24.9|<0.0001|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can refer to page 14 in the our slides \"Logistic Regression.pdf\" if the above table does not show well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the parametric model used in logistic regression?\n",
    "- What is the class label of an individual with a balance equals to 15,000 dollar? What is the class label of an individual with balance equals to 800 dollar? (Write a python function which takes two inputs (feature, model_parameters) and returns the class labels for the data). default is class 1 and non-default is class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression is a parametric model which uses a form of the linear regression .\n",
    "# It uses the LOGIT function which is linear with X( X is one of the classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as math\n",
    "#print(math.exp(3))\n",
    "# def credit(feature,model_parameters):\n",
    "#     num=model_parameters[0]\n",
    "#     for i in range(len(feature)):\n",
    "#          num+=feature[i]*model_parameters[i+1 ]\n",
    "        \n",
    "   \n",
    "#     print(num)\n",
    "#     prob=(1/(math.exp(-num) + 1))\n",
    "#     print(prob)\n",
    "#     if prob>0.5 :\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "# feature1=[800]\n",
    "# feature2=[15000]\n",
    "# model_p1=[-10.6513,0.055 ]\n",
    "# print(model_paramaters1)\n",
    "# #case where feature =800\n",
    "# status1=credit(feature1,model_p1)\n",
    "# print(status1)\n",
    "# status2=credit(feature2,model_p1)\n",
    "# print(status2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.8487\n",
      "Probability is  1.0\n",
      "Induvidual with balance 15000 dollars is 1 \n",
      "\n",
      "-6.251300000000001\n",
      "Probability is  0.0019242363522599306\n",
      "Induvidual with balance o 800 dollars is   0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def def123(feature, model_parameters):\n",
    "    result = model_parameters[0]\n",
    "    for i in range(len(feature)):\n",
    "        #modelparams are the b0 and b1 values ,feature is the balance here\n",
    "        result += feature[i]*model_parameters[i+1]\n",
    "        print(result)\n",
    "    result = (1/(math.exp(-result) + 1))\n",
    "    print('Probability is ',result)\n",
    "    \n",
    "    if(result > 0.5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "     \n",
    "        \n",
    "\n",
    "model_param1 = [-10.6513, 0.0055]\n",
    "feature = [15000]\n",
    "result = def123(feature, model_param1)\n",
    "print('Induvidual with balance 15000 dollars is', result, '\\n')\n",
    "\n",
    "\n",
    "feature = [800]\n",
    "result = def123(feature, model_param1)\n",
    "print('Induvidual with balance o 800 dollars is  ', result, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-2. [10 points]</h4>  <br>\n",
    "\n",
    "The coefficients of logistic regression are obtained by maximizing the likelihood function\n",
    "\n",
    "\\begin{array} \\\\\n",
    "l(\\beta) = \\prod_{i:y_{i}=1} P(y_{i} = 1|x)\\prod_{i{}':y_{{i}'}=0} (1-P(y_{{i}'} = 1|x))\n",
    "\\end{array}\n",
    "Show that maximizing the\n",
    "likelihood function is equivalent to minimizing the cost function $J(\\beta)$, such that.\n",
    "\\begin{array} \\\\\n",
    "J(\\beta) = -\\sum [y_{i} log(P(y_{i} = 1|x)) + (1- y_{i})log(1- P(y_{i} = 1|x))]\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Here $n$ is the number training examples. Mention one possible method for obtaining the\n",
    "optimal coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/file32.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-3 [10 points]</h4> <br>\n",
    "In a fraud detection system, a QDA classifier‚Äôs confusion matrix is found to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        |Predicted Class - Not fraud| Predicted Class - fraud|\n",
    "|:--:|:-------------------------------:|\n",
    "|Actual class ‚Äì Not fraud|1200|25|\n",
    "|Actual class ‚Äì fraud|30|7|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is dataset balanced? Why?\n",
    "- Evaluate the overall error rate <br>\n",
    "- Evaluate the precision and the recall <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/file4.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3>  Problem #4. SVM, Decision Trees, MLP Classification   </h3> \n",
    "### [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will use different classification methods, SVM, Decision Tree and MLP; and find their accuracies using the test data. \n",
    "We will also use the Wisconson **\"breast_cancer\"** dataset.\n",
    "In all of the following subparts, use random_state=0 when you split the dataset into train and test and **standardize** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "  \n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(dataset.data, dataset.target, random_state= 0)\n",
    "\n",
    "# standardize data\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_transformed= scaler.transform(X_train)\n",
    "X_test_transformed= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-1.  Classification with SVM [10 points]</h4><br>\n",
    "- How does the Radial Basis Function Kernel in SVM measure the similarity between a test point and a training example?\n",
    "- Fit an SVM classifier with a radial basis function kernel, with gamma =0.1, and regularization parameter C set to 10. Use the classifier to predict class labels for the test data. \n",
    "- Calculate the accuracy and confusion matrix on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
      " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm1=SVC(kernel='rbf', gamma=0.1, C=10).fit(X_train_transformed,y_train)\n",
    "pred1=svm1.predict(X_test_transformed)\n",
    "print(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52  1]\n",
      " [ 4 86]]\n",
      "0.9885057471264368\n",
      "0.9555555555555556\n",
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,precision_score, recall_score,accuracy_score\n",
    "confusion=confusion_matrix(y_test,pred1)\n",
    "print(confusion)\n",
    "print(precision_score(y_test,pred1))\n",
    "print(recall_score(y_test,pred1))\n",
    "print(accuracy_score(y_test,pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-2.  Classification with Decisin Tree (DT) [10 points]</h4><br>\n",
    "- In this part, use DT classification method on the training data. Set the maximum depth of the tree to five. Then use the classifier to predict class labels for the test data. Calculate the accuracy and confusion matrix on the test data.\n",
    "- Use Adaboost to combine four decision trees each of max_depth of five. Use random_state=0 in adaboost. Find the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9020979020979021\n",
      "[[50  3]\n",
      " [11 79]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeModel=DecisionTreeClassifier(max_depth=5)\n",
    "treeModel.fit(X_train_transformed,y_train)\n",
    "pred2=treeModel.predict(X_test_transformed)\n",
    "print(accuracy_score(y_test,pred2))\n",
    "confusion2=confusion_matrix(y_test,pred2)\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is  0.9370629370629371\n",
      "[[51  2]\n",
      " [ 7 83]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Boostmodel = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth = 5), n_estimators=4, random_state = 0).fit(X_train_transformed, y_train)\n",
    "print('The test accuracy is ', Boostmodel.score(X_test_transformed, y_test))\n",
    "pred3=Boostmodel.predict(X_test_transformed)\n",
    "confusion3=confusion_matrix(y_test,pred3)\n",
    "print(confusion3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-3 [5 points]</h4>\n",
    "\n",
    "Follow steps to answer questions to build a neural network using MLPClassifier from sklearn.neural_network. \n",
    "- Build a model that has two hidden layers, the first layer has 10 neurons and second layer has 5 neurons. \n",
    "- Use 'relu' activation function, and set the regularization parameter alpha=0.5. \n",
    "- Set max_iter=1000; Set the random_state=0.\n",
    "- Use stochastic gradient descent (sgd) to solve the optimization  problem\n",
    "- Report accuracy, confusion matrix, precision, and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is  0.9790209790209791\n",
      "[[52  1]\n",
      " [ 2 88]]\n",
      "0.9887640449438202\n",
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp1=MLPClassifier(solver='sgd', activation='relu', random_state=0,hidden_layer_sizes=[10,5], alpha=0.5,max_iter=1000)\n",
    "mlp1.fit(X_train_transformed, y_train)\n",
    "print('The test accuracy is ', mlp1.score(X_test_transformed, y_test))\n",
    "pred4=mlp1.predict(X_test_transformed)\n",
    "confusion4=confusion_matrix(y_test,pred4)\n",
    "print(confusion4)\n",
    "print(precision_score(y_test,pred4))\n",
    "print(recall_score(y_test,pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4> Problem #4-4 [5 points]</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same setting as problem 4-3, but instead of using two hidden layers, use three hidden layers with 10, 8, 5 hidden neurons respectively.\n",
    "- Find accuracy, precision, recall and confusion matrix. \n",
    "- Comment on the result comparing 4-3 and 4-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is  0.951048951048951\n",
      "[[49  4]\n",
      " [ 3 87]]\n",
      "0.9560439560439561\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp1=MLPClassifier(solver='sgd', activation='relu', random_state=0,hidden_layer_sizes=[10,8,5], alpha=0.5,max_iter=1000)\n",
    "mlp1.fit(X_train_transformed, y_train)\n",
    "print('The test accuracy is ', mlp1.score(X_test_transformed, y_test))\n",
    "pred5=mlp1.predict(X_test_transformed)\n",
    "confusion5=confusion_matrix(y_test,pred5)\n",
    "print(confusion5)\n",
    "print(precision_score(y_test,pred5))\n",
    "print(recall_score(y_test,pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment\n",
    "increasing the number of hidden layers in a neural network doesn't always translate to higher accuracy, as it depends on the complexity of the problem. Increasing layers beyond a certain point will lead to overfitting to the training data. This is why we see lower accuracy in 4.4 compared to 4.3 due to adding an unnecessary hidden layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
